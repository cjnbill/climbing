import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import tempfile
import os
import time

# ================= ğŸ› ï¸ ç¨³å®šå¯¼å…¥ =================
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

# ================= é¡µé¢é…ç½® =================
st.set_page_config(page_title="Climbing AI Coach", page_icon="ğŸ§—", layout="wide")
st.title("ğŸ§— AI Climbing Coach (Action Tagging)")

# ================= ä¾§è¾¹æ è®¾ç½® =================
with st.sidebar:
    st.header("ğŸ”§ Settings")
    processing_speed = st.select_slider(
        "Processing Speed", options=["Standard", "Fast", "Turbo"], value="Fast"
    )
    speed_map = {"Standard": 0, "Fast": 2, "Turbo": 4}
    frame_skip = speed_map[processing_speed]
    
    st.divider()
    st.header("ğŸ“Š Analysis Features")
    show_skeleton = st.checkbox("Show Skeleton", value=True)
    show_metrics = st.checkbox("Tag Key Moves (Speed/Stops)", value=True)
    flag_threshold = st.slider("Flagging Threshold", 130, 170, 150)

# ================= æ ¸å¿ƒç®—æ³• =================
def process_video(input_path, output_path, skip_count):
    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    cap = cv2.VideoCapture(input_path)
    
    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
    
    # ç»Ÿä¸€ç¼©æ”¾åˆ° 720p ä»¥ä¿è¯å¤„ç†é€Ÿåº¦
    target_h = 720
    scale = target_h / orig_h
    width, height = int(orig_w * scale), target_h

    fourcc = cv2.VideoWriter_fourcc(*'VP80') 
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # ç”¨äºè®°å½•å…³é”®åŠ¨ä½œçš„å˜é‡
    hip_history = []  # è®°å½•æœ€è¿‘å‡ å¸§çš„é«‹éƒ¨ä½ç½®è®¡ç®—é€Ÿåº¦
    stops = []        # è®°å½•åœé¡¿ç‚¹ [(x, y, duration), ...]
    last_pos = None
    stop_start_time = None
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    progress_bar = st.progress(0)
    
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        
        frame = cv2.resize(frame, (width, height))
        
        # è·³å¸§é€»è¾‘
        if frame_count % (skip_count + 1) != 0:
            out.write(frame)
            frame_count += 1
            continue

        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(image)
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        if results.pose_landmarks:
            lm = results.pose_landmarks.landmark
            def get_pt(idx): return np.array([lm[idx].x * width, lm[idx].y * height])
            
            # 1. è·å–é‡å¿ƒï¼ˆå·¦å³é«‹éƒ¨ä¸­ç‚¹ï¼‰
            l_hip, r_hip = get_pt(23), get_pt(24)
            curr_hip = (l_hip + r_hip) / 2
            
            if show_metrics:
                # --- å…³é”®åŠ¨ä½œè¯†åˆ«ï¼šé€Ÿåº¦ä¸çˆ†å‘ ---
                if last_pos is not None:
                    dist = np.linalg.norm(curr_hip - last_pos)
                    # å¦‚æœå‘ä¸Šä½ç§»ç¬é—´è¶…è¿‡é˜ˆå€¼ï¼Œåˆ¤å®šä¸ºå‘åŠ›åŠ¨ä½œ
                    if (last_pos[1] - curr_hip[1]) > (height * 0.02): 
                        cv2.putText(image, "POWER MOVE!", (int(curr_hip[0])+20, int(curr_hip[1])), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 3)
                    
                    # --- åœé¡¿ç‚¹æ£€æµ‹ ---
                    if dist < (width * 0.005): # å‡ ä¹æ²¡åŠ¨
                        if stop_start_time is None: stop_start_time = frame_count
                        duration = (frame_count - stop_start_time) / fps
                        if duration > 1.0: # åœé¡¿è¶…è¿‡1ç§’
                            cv2.circle(image, (int(curr_hip[0]), int(curr_hip[1])), 30, (255, 0, 0), 2)
                            cv2.putText(image, f"REST: {duration:.1f}s", (int(curr_hip[0])-40, int(curr_hip[1])-40), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
                    else:
                        stop_start_time = None
                
                last_pos = curr_hip

            # 2. ç»˜åˆ¶éª¨æ¶
            if show_skeleton:
                mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
            
            # 3. è¾…åŠ© Flagging åˆ¤å®š
            l_h, l_k, l_a = get_pt(23), get_pt(25), get_pt(27)
            r_h, r_k, r_a = get_pt(24), get_pt(26), get_pt(28)
            def check_ang(a, b, c):
                rad = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
                ang = np.abs(rad*180.0/np.pi)
                return 360-ang if ang > 180 else ang
            
            if check_ang(l_h, l_k, l_a) > flag_threshold or check_ang(r_h, r_k, r_a) > flag_threshold:
                cv2.putText(image, "NICE FLAG!", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)

        out.write(image)
        frame_count += 1
        if frame_count % 10 == 0: progress_bar.progress(min(frame_count/total_frames, 1.0))

    cap.release()
    out.release()
    progress_bar.empty()
    return output_path

# ================= UI å¸ƒå±€ =================
col1, col2 = st.columns([1, 1])
with col1:
    st.subheader("1. Source Video")
    if 'processed_video' in st.session_state:
        if st.button("ğŸ”„ Analyze New Video"):
            st.session_state.clear()
            st.rerun()
    uploaded_file = st.file_uploader("Upload", type=['mov', 'mp4'])

if uploaded_file:
    if 'original_video' not in st.session_state:
        t = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1])
        t.write(uploaded_file.read())
        st.session_state['original_video'] = t.name
    
    with col1:
        st.video(st.session_state['original_video'])
        if 'processed_video' not in st.session_state:
            if st.button("Analyze Key Moves ğŸš€", type="primary"):
                out_name = tempfile.NamedTemporaryFile(delete=False, suffix='.webm').name
                with col2:
                    with st.spinner('Detecting power moves and rests...'):
                        res = process_video(st.session_state['original_video'], out_name, frame_skip)
                    if res and os.path.getsize(res) > 0:
                        st.session_state['processed_video'] = res
                        st.rerun()

if 'processed_video' in st.session_state:
    with col2:
        st.subheader("2. AI Analysis")
        res_file = st.session_state['processed_video']
        with open(res_file, 'rb') as f: st.video(f.read(), format="video/webm")
        st.download_button("ğŸ“¥ Download Analysis", open(res_file, 'rb'), "climb_analysis.webm")
